\documentclass[twoside,11pt]{article}

\usepackage{style/jmlr2e2}
\usepackage{subcaption}
\usepackage{graphicx}
\graphicspath{ {./images/} }

\jmlrheading{Modified Template}{2021}{}{N/A}{N/A}{}{}

\begin{document}

\title{Machine Learning Scientific Report}

% \author{\name Anonymous author}

% \editor{}

\maketitle

\section{Problem framing}
The Covidâ€‘19 pandemic is still developing today. Between December 2019 and May 2021, there have been over 160 million cases and over 3 million deaths globally. To better identify trends and assist in medical care, we can turn to machine learning (ML) for support.

In this paper, we build and compare predictive models using ML algorithms and epidemiological data from the Covid-19 outbreak. The dataset we use is by \cite{xu2020Epidemiological} and is publicly available on GitHub.\footnote{github.com/beoutbreakprepared/nCoV2019/} The problem we want to solve is of predicting confirmed cases' outcomes as either \emph{died} or \emph{discharged} (from hospital). The motivation for solving this problem is that the prediction model built may be helpful for quickly allocating medical care to patients at a time when there are limited resources. 

\section{Experimental procedure}
\subsection{Data preparation}
First, to better understand the data, we produced a number of visualisations: we created a bar plot of up to the 10 most frequent values of each categorical attribute (e.g., \emph{symptoms}); we created a scatter plot of the \emph{latitude} and \emph{longitude} of each case; and, we plotted the daily and monthly number of cases globally and by country. From these visualisations, we got a strong overview of the spread of Covid-19 by location and by time.

Second, we began preparing the dataset for ML algorithms by removing irrelevant instances. These were instances that were not useful for the task. Since we were training models to predict outcome, irrelevant instances were those whose value of the target, \emph{outcome}, was not one of either \emph{died} or \emph{discharged}. To remove such instances, we had to correct structural errors, such as inconsistent capitalisation and wording among target values: we replaced \emph{Died}, \emph{dead}, or similar values with \emph{died}; and, we replaced \emph{Discharged}, \emph{recovered}, or similar values with \emph{discharged}. Instances with target values whose meaning was dissimilar to either of these two values, including \emph{null} values, were dropped.

Third, we kept attributes which we thought would be useful. The attributes kept, which became features, were \emph{age}, \emph{sex}, \emph{latitude}, \emph{longitude}, \emph{date\_onset\_symptoms}, \emph{symptoms}, \emph{chronic\_disease\_binary}, and \emph{travel\_history\_binary}. After this, we replaced the \emph{symptoms} and \emph{date\_onset\_symptoms} features with a new \emph{symptoms\_binary} feature, and cleaned up the dataset by dropping any remaining instances with missing values. 

Fourth, we transformed numerical data. We performed data binning on the \emph{age} feature, and created categorical values for the age ranges \emph{0-14}, \emph{15-34}, \emph{35-59}, \emph{60-79}, and \emph{80+}. These were chosen because the \emph{age} feature was in fact already categorical, and the latter four categories were the four most frequent. We also performed feature scaling on the \emph{latitude} and \emph{longitude} features by standardisation, also known as Z-score normalisation, which rescales the feature values to have zero-mean and unit-variance.  

Fifth, we transformed categorical data. We performed one-hot encoding on the \emph{age} and \emph{sex} features since they had nominal data. Also, we encode the target values so that \emph{discharged} corresponds to 1 and \emph{died} corresponds to 0. The reason for performing these transformations is that ML algorithms require features to be numbers. 

Finally, we randomise the resulting dataset, which has 5,206 instances, as we do not want the order of the instances, which is irrelevant, to affect the model training process; and, we split the data into a training set, validation set, and test set to prevent overfitting. We create a test set by selecting a 20\% subset of the dataset so there are at least 1,000 test instances, which is large enough to yield statistically meaningful results. Since we suspect the \emph{chronic\_disease\_binary} attribute to be an important feature to predict \emph{outcome}, we chose to ensure that the the test set was representative of the overall \emph{chronic\_disease\_binary} distribution by doing stratified sampling. The overall distribution was that 98.2\% of values were \emph{False} while the remaining 1.8\% were \emph{True}. We also observe that we maintain an exact balance of train and test target values (75.3\% being \emph{discharged}), which makes it easier to train models.

\subsection{Model selection, training, and testing}
The three ML algorithms we chose to build predictive models were logistic regression classification (LRC), gradient boosting classification (GBC), and support vector classification (SVC). For each of these three models, we iteratively train the model on the training set, evaluate the model on the validation set, and tweak the model according to the results on the validation set. In particular, we employ 4-fold cross-validation so that the size of the validation set is equal to the size of the test set. This further reduces the chance of overfitting. Then, we pick the model with the set of hyperparameters that does best on the validation set and confirm the results on the test set. This approach allows us to compare different trained models in an unbiased way, by comparing model performance using the test set, which is kept apart from the training process and is therefore unseen data.

\subsection{Hyperparameter tuning}
We tried different hyperparameter combinations for each model. For LRC, we tried values 0.001, 0.01, 0.1, 1, 10, 100, and 1000 for $C$, the regularisation hyperparameter; and, we tried $L_1$ and $L_2$ regularisation for the norm used in the penalisation. For GBC, we tried values 2, 4, 8, and 16 for the minimum number of samples required to be at a leaf node (\emph{min\_samples\_leaf}); and, values 0.001, 0.01, and 0.1 for the learning rate. And, for SVC, we tried values 0.75, 0.85, 0.95, and 1 for $C$; and, linear, polynomial, radial-basis, and sigmoid kernel types. 

For LRC, the best model achieved a 0.80 accuracy score with hyperparameter combination $C=1000$ and $L_2$ regularisation. For GBC, the best model achieved a 0.85 accuracy score with hyperparameter combination \emph{min\_samples\_leaf}$=2$ and learning rate 0.01. And, for SVC, the best model achieved a 0.84 accuracy score with hyperparameter combination $C=1$ and radial-basis kernel type. 

\section{Results}
Having found the best hyperparameter combinations to train the best models for all three ML algorithms, we then evaluate the trained models on the test set. To enable evaluation, we produce the following: the confusion matrix, positive predictive value (PPV), negative predictive value (NPV), sensitivity, specificity, $F_1$ score, accuracy, and receiver operating characteristic (ROC) curve, for each model.

\begin{table}[!htb]
    \caption{Confusion matrices produced by each trained model on the test set.}
    \begin{subtable}{.33\linewidth}
      \centering
        \caption{LRC.}
        \begin{tabular}{|c|cc|c|}
            \hline
            Pred. & 0 & 1 & all \\
            Real. & & & \\
            \hline
            0 & 185 & 55 & 240\\
            1 & 148 & 654 & 802\\
            \hline
            all & 333 & 709 & 1042\\
            \hline
        \end{tabular}
    \end{subtable}%
    \begin{subtable}{.33\linewidth}
      \centering
        \caption{GBC.}
        \begin{tabular}{|c|cc|c|}
            \hline
            Pred. & 0 & 1 & all \\
            Real. & & & \\
            \hline
            0 & 112 & 128 & 240\\
            1 & 19 & 783 & 802\\
            \hline
            all & 131 & 911 & 1042\\
            \hline
        \end{tabular}
    \end{subtable}%
    \begin{subtable}{.33\linewidth}
        \centering
          \caption{SVC.}
          \begin{tabular}{|c|cc|c|}
            \hline
            Pred. & 0 & 1 & all \\
            Real. & & & \\
            \hline
            0 & 109 & 131 & 240\\
            1 & 24 & 778 & 802\\
            \hline
            all & 133 & 909 & 1042\\
            \hline
        \end{tabular}
      \end{subtable} 
\end{table}

\begin{table}[!htb]
    \caption{Statistics produced by each trained model on the test set.}
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        \textbf{Model} & \textbf{NPV} & \textbf{PPV} & \textbf{Specificity} & \textbf{Sensitivity} & \textbf{$F_1$ Score} & \textbf{Accuracy} \\
        \hline
        LRC & 0.56 & $\mathbf{0.92}$ & $\mathbf{0.77}$ & 0.82 & 0.87 & 0.81 \\
        GBC & $\mathbf{0.85}$ & 0.86 & 0.47 & $\mathbf{0.98}$ & $\mathbf{0.91}$ & $\mathbf{0.86}$ \\
        SVC & 0.82 & 0.86 & 0.45 & 0.97 & $\mathbf{0.91}$ & 0.85 \\
        \hline
    \end{tabular}
    \label{tab:stats}
\end{table}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{lr-roc}
        \caption{LRC.}
        \label{fig:lr-roc}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{gb-roc}
        \caption{GBC.}
        \label{fig:gb-roc}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sv-roc}
        \caption{SVC.}
        \label{fig:sv-roc}
    \end{subfigure}
       \caption{ROC curves produced by each trained model on the test set.}
       \label{fig:roc}
\end{figure}

% \begin{figure}[h]
%     \centering
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{lr-precrec}
%         \caption{LRC.}
%         \label{fig:lr-precrec}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{gb-precrec}
%         \caption{GBC.}
%         \label{fig:gb-precrec}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{sv-precrec}
%         \caption{SVC.}
%         \label{fig:sv-precrec}
%     \end{subfigure}
%        \caption{Three simple graphs}
%        \label{fig:precrec}
% \end{figure}

By looking at the area under the curve (AUC), which is a measure of separability, of the ROC curves in Figure \ref{fig:roc}, we see that GBC produces the highest value followed by LRC then SVC. Since a higher AUC value corresponds to more `idealistic' behaviour (i.e., more correct classifications), we determine that GBC has the best performance for solving our classification problem, followed by LRC then SVC. This ordering is also reflected by the statistics in Table \ref{tab:stats}, which show that GBC performs best in 4 metrics (highlighted in bold), followed by LRC in 2 metrics, and SVC in only 1. 

\section{Discussions}
\subsection{Chosen models}
The reason we chose to implement LRC and GBC is that they were demonstrated in the practical \emph{Exercise for Logistic Regression}\footnote{tinyurl.com/COMP2261prac6} with breast cancer prediction. As such, we base our code on these demonstrations. As for SVC, we chose this algorithm because it was recommended for our problem after consulting the `ML Cheat Sheet' (for scikit-learn)\footnote{scikit-learn.org/stable/tutorial/machine\_learning\_map/index.html}, since we had less than 100k samples and no text data.

\subsection{Experimental procedure}
Although standardisation was useful for LRC and SVC, it does not affect tree based models such as GBC since they are not affected by linear transformations. Nonetheless, standardisation would have helped the model better deal with outliers. However, because we only performed standardisation, we do not know whether min-max normalisation would have produced better results. In future, we can try fitting the model to raw data, min-max normalised data, and standardised data, and compare the results to find the best. 

Another procedure we could have done differently is to split the dataset into train and test sets before performing any data cleaning or transformation. Had we done so, we may have gotten different results. 

\subsection{Limitations}
One limitation of the implementation is that the predictive models use only a handful of attributes that were present in the raw dataset to perform classification. We were going to include the \emph{country} feature in model training; however, we found that the values were skewed towards one country (\emph{Phillipines}), which might affect training performance. Similarly, we wanted to include the \emph{date\_confirmation} attribute; however, our initial exploration of the dataset showed that there were many outliers (spikes in the number of cases around the start or end of each month), and so the attribute might not contain much useful information.

Another limitation of the implementation is that we do not explicitly perform down sampling or up-weighted during model training to account for the imbalanced data set, since \emph{discharged} accounts for about 75\% of \emph{outcome} values. Had we done this, we may have observed fewer false positives or false negatives in the confusion matrix.

Finally, we could have included more hyperparameters to try in our combinations, such as the deviance and exponential loss functions to be optimised for GBC and degree of the polynomial kernel function for SVC.

\section{Conclusions and lessons learnt}
Overall, we conclude that GBC is the best ML algorithm at solving our problem of predicting confirmed cases' outcomes as either \emph{died} or \emph{discharged}. Moreover, we learned the lesson that we can never conduct enough experiments in ML, since there are always more algorithms to try, attributes to select, and hyperparameters to tune.

% \vskip 0.2in
\bibliography{references}

\end{document}