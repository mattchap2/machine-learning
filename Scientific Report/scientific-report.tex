\documentclass[twoside,11pt]{article}

\usepackage{style/jmlr2e2}
\usepackage{subcaption}
\usepackage{graphicx}
\graphicspath{ {./images/} }

\jmlrheading{Modified Template}{2021}{}{N/A}{N/A}{}{}

\begin{document}

\title{Machine Learning Scientific Report}

% \author{\name Anonymous author}

% \editor{}

\maketitle

\section{Problem framing}
The Covidâ€‘19 pandemic is still developing today. Between December 2019 and May 2021, there have been over 160 million cases and over 3 million deaths globally. To better identify trends and assist in medical care, we can turn to machine learning (ML) for support.

In this paper, we build and compare predictive models using ML algorithms and epidemiological data from the Covid-19 outbreak. The dataset we use is by \cite{xu2020Epidemiological} and is publicly available on GitHub.\footnote{github.com/beoutbreakprepared/nCoV2019/} The problem we want to solve is of predicting confirmed cases' outcomes as either \emph{died} or \emph{discharged} (from hospital). The motivation for solving this problem is that the prediction model built may be helpful for quickly allocating medical care to patients at a time when there are limited resources. 

\section{Experimental procedure (35\%)}
To better understand the data, we produce a number of visualisations: we create a bar plot of up to the 10 most frequent values of each categorical attribute (e.g., \emph{symptoms}); we create a scatter plot of the \emph{latitude} and \emph{longitude} of each case; and, we plot the daily and monthly number of cases globally and by country (e.g., China). From these visualisations, we get a strong overview of the spread of COVID-19 by location and by time. Next, we prepare the dataset for use by ML algorithms.

\subsection{Data preparation}
First, we removed irrelevant instances. These were instances that were not useful for the task. Since we were training models to predict patient outcome, irrelevant instances were those whose value of the target, \emph{outcome}, was not one of either \emph{died} or \emph{discharged}. To remove such instances, we had to correct structural errors, such as inconsistent capitalisation and wording among target values. We replaced the \emph{Died}, \emph{dead}, or similar values with \emph{died}; and, we replaced the \emph{Discharged}, \emph{recovered}, or similar values with \emph{discharged}. Instances with target values whose meaning was dissimilar to either of these two values, including \emph{null} values, were dropped.

Second, we kept attributes which we thought would be useful. The attributes kept, which became features, were \emph{age}, \emph{sex}, \emph{latitude}, \emph{longitude}, \emph{date\_onset\_symptoms}, \emph{symptoms}, \emph{chronic\_disease\_binary}, and \emph{travel\_history\_binary}. After this, we replaced the \emph{symptoms} and \emph{date\_onset\_symptoms} features with a new \emph{symptoms\_binary} feature, and cleaned up the dataset by dropping any remaining instances with missing values. 

Third, we transformed numerical data. We performed data binning on the \emph{age} feature, and created categorical values for the age ranges \emph{0-14}, \emph{15-34}, \emph{35-59}, \emph{60-79}, and \emph{80+}. These were chosen because the \emph{age} feature was in fact already categorical, and the latter four categories were the four most frequent. We also performed feature scaling on the \emph{latitude} and \emph{longitude} features by standardisation, also known as Z-score normalisation, which rescales the feature values to have zero-mean and unit-variance.  

Fourth, we transformed categorical data. We performed one-hot encoding on the \emph{age} and \emph{sex} features since they had nominal data. Also, we encode the target values so that \emph{discharged} corresponds to 1 and \emph{died} corresponds to 0. The reason for performing these transformations is that ML algorithms require features to be numbers. 

Finally, we randomise the resulting dataset, which has 5,206 instances, as we do not want the order of the instances, which is irrelevant, to affect the model training process; and, we split the data into a training set, validation set, and test set to prevent overfitting. We create a test set by selecting a 20\% subset of the dataset so there are at least 1,000 test instances, which is large enough to yield statistically meaningful results. Since we suspect the \emph{chronic\_disease\_binary} attribute to be an important feature to predict \emph{outcome}, we chose to ensure that the the test set was representative of the overall \emph{chronic\_disease\_binary} distribution by doing stratified sampling. The overall distribution was that 98.2\% of values were \emph{False} while the remaining 1.8\% were \emph{True}. We also observe that we maintain an exact balance of train and test target values (75.3\% being \emph{discharged}), which makes it easier to train models.

\subsection{Model selection, training, and testing}
The three ML algorithms we chose to build predictive models were the logistic regression classifier (LRC), gradient boosting classifier (GBC), and support vector classifier (SVC). For each of the 3 models, we iteratively train the model on the training set, evaluate the model on the validation set, and tweak the model according to the results on the validation set. In particular, we employ 4-fold cross-validation so that the size of the validation set is equal to the size of the test set. This reduces the chance of overfitting. Then, we pick the model with the set of hyperparameters that does best on the validation set and confirm the results on the test set. This approach allows us to compare different trained models in an unbiased way, by comparing model performance using the test set, which is kept apart from the training process and is therefore unseen data.

\subsection{Hyperparameter tuning}
We tried different parameter combinations for each model. For the LRC, we tried values 0.001, 0.01, 0.1, 1, 10, 100, and 1000 for $C$, the regularisation parameter; and, we tried $L_1$ and $L_2$ regularisation for the norm used in the penalisation. For the GBC, we tried values 2, 4, 8, and 16 for the minimum number of samples required to be at a leaf node; and, values 0.001, 0.01, and 0.1 for the learning rate. For the SVC, we tried values 0.75, 0.85, 0.95, and 1 for $C$; linear, polynomial, radial-basis, and sigmoid kernel types; and, values 3, 4, and 5 for the degree of the polynomial kernel function. We present the results from these models in the next section.

\section{Results (25\%)}
In this section, we present the confusion matrix for each of the three models, the precision and recall tables, as well as  the ROC curves.

For the linear regression classifier, the best accuracy score was 0.80, with $C=10$ and $L_2$ regularisation. For the gradient boosting classifier, the best accuracy score was 

\begin{table}[!htb]
    \caption{Confusion matrices for each predictive model.}
    \begin{subtable}{.33\linewidth}
      \centering
        \caption{LRC.}
        \begin{tabular}{|c|cc|c|}
            \hline
            Pred. & 0 & 1 & all \\
            Real. & & & \\
            \hline
            0 & 185 & 55 & 240\\
            1 & 148 & 654 & 802\\
            \hline
            all & 333 & 709 & 1042\\
            \hline
        \end{tabular}
    \end{subtable}%
    \begin{subtable}{.33\linewidth}
      \centering
        \caption{GBC.}
        \begin{tabular}{|c|cc|c|}
            \hline
            Pred. & 0 & 1 & all \\
            Real. & & & \\
            \hline
            0 & 112 & 128 & 240\\
            1 & 19 & 783 & 802\\
            \hline
            all & 131 & 911 & 1042\\
            \hline
        \end{tabular}
    \end{subtable}%
    \begin{subtable}{.33\linewidth}
        \centering
          \caption{SVC.}
          \begin{tabular}{|c|cc|c|}
            \hline
            Pred. & 0 & 1 & all \\
            Real. & & & \\
            \hline
            0 & 109 & 131 & 240\\
            1 & 24 & 778 & 802\\
            \hline
            all & 133 & 909 & 1042\\
            \hline
        \end{tabular}
      \end{subtable} 
\end{table}

\begin{table}[!htb]
    \caption{Global caption}
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        \textbf{Model} & \textbf{NPV} & \textbf{PPV} & \textbf{Specificity} & \textbf{Sensitivity} & \textbf{F1 Score} & \textbf{Accuracy} \\
        \hline
        LRC & 0.56 & $\mathbf{0.92}$ & $\mathbf{0.77}$ & 0.82 & 0.87 & 0.81 \\
        GBC & $\mathbf{0.85}$ & 0.86 & 0.47 & $\mathbf{0.98}$ & $\mathbf{0.91}$ & $\mathbf{0.86}$ \\
        SVC & 0.82 & 0.86 & 0.45 & 0.97 & $\mathbf{0.91}$ & 0.85 \\
        \hline
    \end{tabular}
\end{table}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{lr-roc}
        \caption{LRC.}
        \label{fig:lr-roc}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{gb-roc}
        \caption{GBC.}
        \label{fig:gb-roc}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sv-roc}
        \caption{SVC.}
        \label{fig:sv-roc}
    \end{subfigure}
       \caption{Three simple graphs}
       \label{fig:roc}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{lr-precrec}
        \caption{LRC.}
        \label{fig:lr-precrec}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{gb-precrec}
        \caption{GBC.}
        \label{fig:gb-precrec}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{sv-precrec}
        \caption{SVC.}
        \label{fig:sv-precrec}
    \end{subfigure}
       \caption{Three simple graphs}
       \label{fig:precrec}
\end{figure}


\begin{itemize}
    \item Make comparisons between the 3 predictive models
    \item Provide necessary tables and charts to summarise and support the comparisons.
\end{itemize}

\section{Discussions (20\%)}
\subsection{Chosen models}
We chose the first two algorithms because they were demonstrated in \emph{Exercise for Logistic Regression}\footnote{tinyurl.com/COMP2261prac6} with breast cancer prediction, and the third because it is recommended by the ML Cheat Sheet (for scikit-learn)\footnote{scikit-learn.org/stable/tutorial/machine\_learning\_map/index.html}.
\subsection{Experimental procedure}
\begin{itemize}
    \item standardisation is commonly used by KNN, SVM, PCA, etc. but not necessary for logistic reg, tree-based, and random forest
    \item standardisation helps better deal with outliers, but min-max can generate smaller std; could have tried fitting model to raw, normalised, and standardised data da the ncompare their performances for the best results
    \item could have imputed
\end{itemize}
\subsection{Limitations}
\begin{itemize}
    \item Was going to include `country', then saw that data over represents Philippines
    \item Further work can include date\_confirmation
    \item We notice we have an imbalanced data set, so we employ down sampling and up-weighting during model training.
    \item Larger k means less (pessimistic) bias. Leave-One-Out Cross-Validation is too computationally expesive.
    \item  deviance and exponential loss functions to be optimised for gradient boosting
\end{itemize}

\section{Conclusions and lessons learnt (10\%)}
\begin{itemize}
    \item Discuss the results and draw conclusions from your experimentation
\end{itemize}




\vskip 0.2in
\bibliography{references}

\end{document}