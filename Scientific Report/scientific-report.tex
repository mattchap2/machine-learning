\documentclass[twoside,11pt]{article}

\usepackage{style/jmlr2e}

\begin{document}

\title{Summative Assignment\\ 
Artificial Intelligence COMP2261 –\\
Machine Learning 2020/2021}

\maketitle

\section{Problem framing (10\%)}
Coronavirus (COVID‑19) is spreading fast. Since first reported in December 2019, by mid-January 2021, it has affected more than 95 million people and killed more than 2 million people worldwide. To aid the analysis and inform public health decision making, machine learning models trained on real data can be very useful.

In this paper, we build and compare predictive models using machine learning algorithms and epidemiological data from the COVID-19 outbreak. We explore the dataset and aim to solve the problem of predicting patient outcome as either “died” or “discharged” from hospital. The motivation is that the proposed prediction model may be helpful for the quick triage of patients without having to wait for the results of additional tests such as laboratory or radiologic studies, during a pandemic when limited medical resources must be wisely allocated without hesitation. 

\section{Experimental procedure (35\%)}
The experimental procedure was to follow the machine learning workflow, which consists of 7 stages: problem framing, data preparation, model selection, model training, model testing, hyperparameter tuning, and inference/prediction. 

\subsection{Data preparation}
We used the dataset by \cite{xu2020Epidemiological} which is publicly available on GitHub\footnote{github.com/beoutbreakprepared/nCoV2019/}.

First, we removed irrelevant instances. These are instances that are not useful for the task. Since we are training models to predict patient outcome, irrelevant instances had `null' as the outcome. We also corrected structural errors, such as inconsistent capitalisation. We made one category for `died', `Died', `dead', or similar, and another for `discharged', `Discharged', `recovered', or similar. Instances which did not fall under either one of these two categories were dropped. 

Second, we kept attributes which we thought could be useful to the models. The attributes kept were `age', `sex', `country', `latitude', `longitude', `date\_onset\_symptoms', `date\_confirmation', `symptoms', `chronic\_disease\_binary', and `travel\_history\_binary'. After this, we cleaned up the data by replacing `date\_onset\_symptoms' and `symptoms' with a new attribute, `symptoms\_binary', then dropping any remaining instances with missing values. 

Third, we transformed numeric data. We perform data binning on the `age' attribute. The ranges were `0-14', `15-34', `35-59', `60-79', and `80+'. These were chosen because the `age' attribute was in fact already categorical, and the latter 4 ranges were the 4 most frequent categories. We also perform standardisation on the `latitude' and `longitude' attributes. (remove country)

Fourth, we transformed categorical data.

\subsubsection{Data sampling}

We notice we have an imbalanced data set, so we employ down sampling and up-weighting during model training.

\subsubsection{Data splitting}
We split the dataset into training set and test set. We keep them separate, as we don't want the model to memorise. Before splitting, we randomise the dataset as we don't want the order of the instances, which is irrelevant, to affect the model training process. We make our test set meet two conditions: it is large enough to yield statistically meaningful results, and it is representative of the dataset as a while.

To prevent overfitting, we produce a validation set. We train the moddel on the training set, then evaluate the model on the validation set and use those results to tweak the odel iteratively. We leave the test set separate to only confirm results of the model that does best on the validation set. This creates fewer exposures to the test set.

We employ k-fold cross validation to reduce the chance of overfitting, assess how well the model performs on previously unseen data, and resampling producedure to tes models on a limited data sample

\subsubsection{Data transformation}
Before feature transformation we explore and clean up data and visualise data in graphs and charts.

We perform data transformation for data compatibiltiy and better model performance.

Numeric data. We perform binning on age. We perform feature scaling because we will be using SVM, kNN, PCA, clustering (not necessary for logistic regression, decision tree). 

categorical data. We encode categorical data in roder to be able to fit and evaluaet models. We use one-hot encoding

\subsection{Model selection}
We chose SVM, kNN, and logistic reg.

\subsection{Model training}
\subsection{Model testing}
\subsection{Hyperparameter tuning}
\subsection{Inference/Prediction}
\begin{itemize}
    \item Clean the dataset.
    \item Split the dataset into training and test sets.
    \item Train a logistic regression model.
    \item Train a polynomial regression model.
    \item Train a normal regression model.
\end{itemize}

\section{Results (25\%)}
\begin{itemize}
    \item Make comparisons between the 3 predictive models
    \item Provide necessary tables and charts to summarise and support the comparisons.
\end{itemize}

\section{Discussions (20\%)}
\subsection{Chosen models}
\subsection{Experimental procedure}
\subsection{Limitations}

\section{Conclusions and lessons learnt (10\%)}
\begin{itemize}
    \item Discuss the results and draw conclusions from your experimentation
\end{itemize}




\vskip 0.2in
\bibliography{references}

\end{document}